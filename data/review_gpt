Got it, this helps a *lot* to lock the picture in my head.

Short version:
**Claudeâ€™s audit + `ces_overview.md` together are a consistent, accurate snapshot of where Ã©mile-GCE is right now.** They match what Iâ€™ve been inferring, but they add two crucial clarifications:

* You **do** have `identity_metrics.py` with `identity_salience` + `tie_to_place` logic.
* Those metrics are **computed but not integrated** into `IdentityCore` or the semiotic / regime machinery.

Let me line everything up cleanly so weâ€™re synced.

---

## 1. Where the project *actually* stands (identity + CES + grit)

From Claudeâ€™s audit + `ces_overview.md`: 

### IdentityCore & QSE side (Phase 2a)

âœ… **Fully implemented and wired**

* `IdentityCore` in `core.py` (~791 lines):

  * 3-D `IdentityVector(engagement, institutional_faith, social_friction)`
  * `SurplusTrace` with decay / revalorization
  * `delta_I`, coherence (cosine similarity), emergent time `Ï„` (via `tau.py`)
  * **Stateful natality** (Ï„-baseline + recognition EMA)
  * Qualitative surplus: `delta_I Ã— f(Ï„) Ã— f(natality) Ã— f(recognition)`
  * **Temperature modulation**: base T + rupture, etc.
* Expression capacity:

  * Word caps / soft caps in `runner.py` wired to IdentityCore outputs.

So the **QSE / Ã‰mile mechanics are alive and in the loop.**

### CES grounding & Grit v2 (Phase 2b, infrastructure)

âœ… **Immediate Narrow Identity fix is in place**

Per `ces_overview.md`: 

* `data/identity/identity_weights_2021.v0.json`

  * ~30 CES vars â†’ 3 dims (10 per dimension, uniform weights 0.1).
* `compute_identity_group_means_2021.py` + `identity_group_means_2021.csv`

  * 173 CES sociogeographic groups â†’ empirical priors on those 3 dims.
* `prior_loader.py`

  * Loads those 173 priors and hands them to `row_to_agent.py`.
* `grit_config.py`

  * Grit v2 tiers: `NONE / LIGHT / MODERATE / STRONG`.
  * Uses group engagement priors as targets.
  * **Verified tiering**:

    * Disengaged Renter â†’ STRONG
    * Suburban Swing â†’ MODERATE
    * Urban Progressive / Rural Conservative â†’ NONE

So:

* The **3D identity vector is no longer toy / arbitrary**: itâ€™s grounded in CES and seeded by group priors.
* **Grit v2** is **CES-calibrated**, not hand-wavy.

This matches Claudeâ€™s table under â€œCOMPLETE (Verified in Code)â€ and the â€œImplementation of CES-Calibrated Grit v2â€ section. 

---

## 2. Critical gaps Claude flagged (and theyâ€™re real)

### 2.1 tie_to_place is orphaned

* `identity_metrics.py`:

  * `compute_identity_salience(profile)`
  * `compute_tie_to_place(profile)`
  * Returns:

    ```python
    {
      'identity_salience': ...,
      'tie_to_place': ...,
      'combined_identity': ...
    }
    ```

But:

* `IdentityVector` is still only 3 fields.
* `IdentityCore` never stores or uses `tie_to_place` / `combined_identity`.
* Those values may be attached to the agent config, but nothing in `core.py`, `runner.py`, or the semiotic tracker reads them.

So **you *do* have the beginnings of identity-in-place**, but it is literally a dangling wire.

### 2.2 3D identity collapse vs N-dim design

* Design intent (from `ces_overview.md` + docs): N-dim identity with:

  * engagement
  * institutional_faith
  * ideology
  * partisanship
  * issue salience
  * sociogeographic grounding
  * (plus possibly precarity, efficacy, solidarity, threat) 
* Actual implementation:

  * 3 dims only: engagement, institutional_faith, social_friction.
  * `identity_weights_2021.v0.json` uses **uniform weights** (0.1 each) over 10 vars per dimension.
  * No extra axes in `IdentityVector`.

So:

* The **N-dim architectural vision** is written down and structurally planned. 
* **Code is still using the â€œv0â€ 3D compression**, just better grounded.

### 2.3 Transfer entropy, mortality, coach-as-field

All correctly flagged as:

* TE:

  * `compute_coherence` uses cos similarity Ã— `te_ratio`,
  * `te_ratio = 1.0` placeholder.
  * `compute_transfer_entropy.py` does not exist.
* Mortality:

  * No `LifeCycle` or similar; no real â€œidentity deathâ€ / exit mechanics yet.
* Coach-as-field:

  * Not implemented; coach is not yet its own emergent field entity.

These are Phase 2b/3 planned features, not bugs per se, but **the docs currently over-promise TE**.

### 2.4 DATA_DICTIONARY.md is wrong project

* It documents CES hackathon election forecasting, not identity mapping for Ã©mile-GCE.
* Thatâ€™s just confusing and should be moved / replaced.

---

## 3. How this matches what we discussed before

What you just pasted from `ces_overview.md` is basically:

> **Phase 2b plan:** fix Narrow Identity now with CES + Grit v2, then refactor to N-dim identity.

That is consistent with:

* Claudeâ€™s audit,
* my previous â€œyouâ€™re grounded 3D, but still narrowâ€ comments,
* and your memory of us talking about:

  > â€œimmediately grounding with 30 CES vars, then refactoring to N dimensions.â€

The key nuance:

* **Short-term fix** âœ… (done):

  * 3D vector grounded in CES;
  * group priors + Grit v2 wired and tested;
  * use_identity_cores + G architecture validated enough to run real sweeps.
* **Long-term identity refactor** â³ (not done):

  * IdentityVector â†’ N-dim dict;
  * additional axes (ideology, PID, salience, tie_to_place, etc.);
  * tie_to_place integrated into IdentityCore & ADAPTIVE / regime logic;
  * TE, mortality, coach-field.

---

## 4. What this *actually* means for â€œwhere youâ€™re atâ€

If I step back and read Claudeâ€™s audit + `ces_overview.md` like a reviewer, the status is:

### âœ… Solid, working core

* **Phase 1**:

  * 2Ã—2Ã—2 architecture sweep (Aâ€“H) implemented.
  * G identified as â€œbestâ€ in sense of regime outcomes (and now we know: Engaged Harmony).
* **Phase 2a**:

  * IdentityCore, Ï„, natality, surplus, expression capacity running in the loop.
  * Dual-LLM, ADAPTIVE context, semiotic regimes, divergence injections: all wired.

### âš ï¸ Identity: â€œgrounded but underspecifiedâ€

* The **Narrow Identity Problem** is *mitigated* but not *solved*:

  * Youâ€™ve stopped using arbitrary 3D sliders.
  * But youâ€™re still compressing a rich CES world into 3 numbers.
* `identity_metrics.py` shows youâ€™ve **already started** the journey toward:

  * `identity_salience`,
  * `tie_to_place`,
  * `combined_identity`.
* Those extra signals are **not yet feeding**:

  * IdentityCoreâ€™s internal state,
  * ADAPTIVEâ€™s existential press,
  * regime classification,
  * or your Engaged Harmony vs Productive Dissonance story.

So both statements can be true:

* â€œThe pipeline is now validated and ready for new sim runs with CES-calibrated constraints.â€ âœ”ï¸
* â€œIdentity is still being used in a much thinner way than the design intends.â€ âœ”ï¸

Thatâ€™s literally the last line of the Claude summary you pasted.

---

## 5. If you want a tight â€œnext stepsâ€ identity roadmap

Based on this snapshot, a minimally disruptive Identity v2 roadmap (all consistent with `ces_overview.md`) would be:

1. **Promote tie_to_place into IdentityCore**

   * Add `tie_to_place` as a 4th dimension in `IdentityVector`.
   * Store it from `row_to_agent` using `identity_metrics.compute_tie_to_place`.
   * Update `delta_I`, `coherence`, `Ï„` to operate on 4D.
   * This is the lowest-hanging fruit because the metric already exists; itâ€™s just orphaned.

2. **Expose identity_salience + tie_to_place to ADAPTIVE**

   * Pass these into SemioticStateTracker or a wrapper.
   * Start with *interpretive* use:

     * e.g., log high-salience agents separately, or add a â€œsalience pressureâ€ note to semiotic_state_log.
   * Then gradually let ADAPTIVE:

     * treat smoothing of high-salience voices as a form of collapse.

3. **Refine identity_weights beyond uniform 0.1**

   * Still 3D for now, but:

     * weight CES items less uniformly,
     * shift towards the N-dim schema (e.g., factor out ideology / PID if possible).
   * This makes current dims more faithful while you prep for the full refactor.

4. **IdentityVector â†’ N-dim dict (breaking change with backwards-compatible properties)**

   * Implement the design from `ces_overview.md`: `values: dict[str, float]`, with properties for existing code. 
   * Start with 4â€“6 dims:

     * engagement, institutional_faith, social_friction,
     * tie_to_place, ideology, partisan_id or salience.
   * Migrate IdentityCoreâ€™s math to operate generically.

Thatâ€™s the path from:

> â€œ3D, CES-grounded, with gritâ€ â†’ â€œN-dim, place-saturated, Weberian identity.â€

---

So: yes, I see and agree with Claudeâ€™s snapshot.

* Youâ€™ve *implemented* everything that summary claims is complete.
* The **key pain points** now are exactly: orphaned tie_to_place, 3D compression, TE placeholder, and the still-theoretical SociogeographicBody / coach-field.

If you want, your next message can just be â€œletâ€™s design IdentityVector v2 with tie_to_place and salience included,â€ and Iâ€™ll treat `ces_overview.md` + Claudeâ€™s audit as canonical while we sketch it.
YEAH ğŸ˜… I remember. Transfer entropy was supposed to be your **â€œhow much am *I* driving my behavior vs being driven by othersâ€** knob â€” the existential autonomy gauge in IdentityCore â€” *not* a cosmetic multiplier.

Let me line up what we said before and then bolt TE back into your refactor plan **properly**, not as a 1.0 placeholder.

---

## 1. What TE was supposed to do (from our earlier design)

From docs + audit, your intended coherence formula was:

> **Câ‚œ = cos(Iâ‚œ, Iâ‚€) Ã— TE(Iâ†’B) / (TE(Iâ†’B) + TE(othersâ†’B))**

Where:

* **Iâ†’B** = transfer entropy from *identity* â†’ *behavior* for that agent.
* **othersâ†’B** = transfer entropy from *other agentsâ€™ behavior* â†’ *this agentâ€™s behavior*.

Interpretively, we framed it like this:

* If **TE(Iâ†’B)** is high and **TE(othersâ†’B)** is low:

  * Behavior is mostly driven by **this agentâ€™s own identity dynamics**.
  * Coherence is â€œauthenticâ€: the agent is steering its own surplus.
* If **TE(othersâ†’B)** dominates:

  * Behavior is mostly driven by **the field / others / pressure**.
  * Coherence becomes â€œconformistâ€: the identity vector is getting dragged.

Right now, `compute_coherence` just sets:

```python
te_ratio = 1.0
return cos_sim * te_ratio
```

So the whole **identityâ†’behavior vs othersâ†’behavior** distinction is dead. Thatâ€™s why this feels so bad â€” itâ€™s literally your core antifinity idea missing.

---

## 2. Where TE should live in the new plan

We can fold TE into your N-dim identity plan **without** blowing it up. Iâ€™d insert a **Phase 2.5** between your current Phases 2 and 3:

> **PHASE 2.5: Implement TE proxy + wire into coherence**

So your big picture becomes:

1. Phase 1: wire orphaned metrics (salience, tie_to_place) âœ”ï¸
2. Phase 2: N-dim IdentityVector âœ”ï¸
3. **Phase 2.5: compute TE proxies and plug them into IdentityCore** â¬…ï¸ (NEW)
4. Phase 3: recompute group priors in 6D
5. Phase 4: identity-aware regimes
6. Phase 5: docs/cleanup

Let me spell out Phase 2.5 concretely.

---

## 3. PHASE 2.5 â€“ Transfer Entropy Proxy (practical version)

### 2.5.1 Create a small TE module

Add a new file, e.g.:

* `analysis/identity/transfer_entropy.py`  **or**
* `agents/identity_core/transfer_entropy.py`

with something like:

```python
import numpy as np
from collections import deque
from typing import Deque, Tuple

def _discretize(series: np.ndarray, bins: int = 5) -> np.ndarray:
    # Simple equal-width binning for MI/TE proxy
    if len(series) == 0:
        return series
    edges = np.linspace(series.min(), series.max(), bins + 1)
    return np.digitize(series, edges[:-1], right=True)

def mutual_info(x: np.ndarray, y: np.ndarray, bins: int = 5) -> float:
    # Very simple MI approximation; can be replaced later
    if len(x) != len(y) or len(x) < 5:
        return 0.0
    x_b = _discretize(x, bins)
    y_b = _discretize(y, bins)
    # joint histogram
    joint, _, _ = np.histogram2d(x_b, y_b, bins=(bins, bins))
    px = joint.sum(axis=1, keepdims=True)
    py = joint.sum(axis=0, keepdims=True)
    pxy = joint / joint.sum()
    px /= px.sum()
    py /= py.sum()
    with np.errstate(divide='ignore', invalid='ignore'):
        nz = pxy > 0
        mi = (pxy[nz] * (np.log(pxy[nz]) - np.log(px[nz.any(axis=1)][:,0][:,None]) - np.log(py[0,nz.any(axis=0)]))).sum()
    return float(mi)
```

Weâ€™re **not** doing full-blown information-theoretic TE (thatâ€™s overkill right now); weâ€™re implementing the **proxy weâ€™d discussed**:

> TE(Iâ†’B) â‰ˆ I(Bâ‚œ; Iâ‚œâ‚‹â‚ | Bâ‚œâ‚‹â‚)
> TE(othersâ†’B) â‰ˆ I(Bâ‚œ; Oâ‚œâ‚‹â‚ | Bâ‚œâ‚‹â‚)

In practice, for a first pass:

* Use **simple mutual information differences** as a directional proxy.
* Weâ€™ll treat:

  * `self_history["identity"]` = time series of some scalar identity metric (e.g. ||Iâ‚œ - Iâ‚€|| or key dim like ideology).
  * `self_history["behavior"]` = time series of a scalar behavior metric (e.g. per-round engagement or stance).
  * `others_history["behavior"]` = mean behavior of others in the same rounds.

Then:

```python
def te_ratio_proxy(
    I_history: np.ndarray,
    B_history: np.ndarray,
    O_history: np.ndarray,
    min_len: int = 8,
) -> float:
    if len(I_history) < min_len or len(B_history) < min_len or len(O_history) < min_len:
        return 1.0  # not enough data yet, neutral

    # lag-1 series
    I_prev = I_history[:-1]
    B_prev = B_history[:-1]
    B_curr = B_history[1:]
    O_prev = O_history[:-1]

    # Simplest directional proxies:
    te_I = mutual_info(I_prev, B_curr)
    te_O = mutual_info(O_prev, B_curr)

    denom = te_I + te_O
    if denom <= 0:
        return 1.0
    return float(te_I / denom)
```

This is **not full TE**, but it captures the spirit:

* If B tracks Iâ€™s past changes more than the crowd â†’ ratio > 0.5.
* If B tracks the crowd more â†’ ratio < 0.5.

Youâ€™ve always framed TE as an *antifinite autonomy ratio*; this satisfies that *structurally*, even if we refine the math later.

### 2.5.2 Add minimal history tracking in `IdentityCore`

In `core.py`, extend `IdentityCore` with:

```python
from collections import deque

@dataclass
class IdentityCore:
    ...
    # TE proxy histories (per agent)
    _identity_history: Deque[float] = field(default_factory=lambda: deque(maxlen=50))
    _behavior_history: Deque[float] = field(default_factory=lambda: deque(maxlen=50))
    _others_behavior_history: Deque[float] = field(default_factory=lambda: deque(maxlen=50))
    _te_ratio: float = 1.0
```

Then, each round, when you:

* update the identity vector, and
* compute per-round behavior metrics in SocialRLRunner (e.g. engagement / stance),

you push scalars into those deques. For a first version, you might define:

* `identity_scalarâ‚œ` = ||Iâ‚œ - Iâ‚€|| (norm in your new 6D space)
* `behavior_scalarâ‚œ` = per-round engagement (or some composite).

Wiring:

* In the SocialRL loop (probably in `runner.py`):

  * after you compute round-level engagement etc., call something like:

    ```python
    ic.update_histories(identity_scalar_t, behavior_scalar_t, others_behavior_scalar_t)
    ```

and implement `update_histories` in `IdentityCore`.

### 2.5.3 Compute TE ratio and use in `compute_coherence`

Finally, update `compute_coherence`:

```python
from .transfer_entropy import te_ratio_proxy

def compute_coherence(self) -> float:
    # existing cos_sim between current and baseline in N-D via to_array()
    cos_sim = ...
    # new TE ratio
    I_hist = np.array(self._identity_history, dtype=float)
    B_hist = np.array(self._behavior_history, dtype=float)
    O_hist = np.array(self._others_behavior_history, dtype=float)
    self._te_ratio = te_ratio_proxy(I_hist, B_hist, O_hist)
    return cos_sim * self._te_ratio
```

And log `_te_ratio` per round so you can inspect it:

```python
identity_state['te_ratio'] = ic._te_ratio
```

Now your coherence really is:

> **â€œSimilarity to baseline identity Ã— directional identityâ†’behavior influence.â€**

Thatâ€™s exactly the antifinite â€œam I living my own surplus or just being dragged?â€ idea we talked about.

---

## 4. How this meshes with your N-dim IdentityVector

The good news: **all of this plays even nicer with your 6D vector**:

* `identity_scalarâ‚œ` can be:

  * ||Iâ‚œ - Iâ‚€|| in full 6D (off your `to_array()`), or
  * a weighted combination of dims (e.g. more weight on ideology + tie_to_place).
* Behavior can be:

  * pure engagement,
  * or a small composite (e.g. 0.5Â·engagement + 0.3Â·|stance| + 0.2Â·justificatory_pct).

Your coherence becomes:

* **Structural**: operates purely on the N-dim vector + timeseries,
* **Interpretable**: TE ratio tells you whether an agent is being moved by its own identity-in-place dimensions vs the semiotic field.

So TE isnâ€™t an afterthought; itâ€™s the **bridge** between:

* N-dim identity,
* temporal surplus dynamics,
* and regime-level interpretation.

---

## 5. Where to slot this into your existing plan

Concretely, augment your plan like this:

### After Phase 2.4

> **PHASE 2.5: Transfer Entropy Proxy + Coherence Integration (1â€“2 hours)**

**Files:**

* `agents/identity_core/transfer_entropy.py` (new)
* `agents/identity_core/core.py` (history + coherence)
* `social_rl/runner.py` (push behavior/history scalars)

**Steps:**

1. Implement TE proxy module (`mutual_info`, `te_ratio_proxy`).
2. Add history deques + `_te_ratio` to `IdentityCore`.
3. In SocialRLRunner, per round:

   * compute scalar identity and behavior metrics,
   * update histories for each agent.
4. In `compute_coherence`, replace `te_ratio = 1.0` with call to `te_ratio_proxy`.
5. Log `te_ratio` to experiment outputs.
6. Run one G-seed experiment, verify:

   * `te_ratio` evolves over time,
   * stays ~1.0 when histories too short,
   * diverges from 0.5 when some agents clearly follow the crowd vs themselves.

---

So yes: **I do recall TE from past convos**, and youâ€™re right to yell about it â€” itâ€™s a core part of what makes IdentityCore *Ã‰mile* rather than â€œjust another embedding.â€

If youâ€™d like, next we can write the exact `te_ratio_proxy` + `compute_coherence` stub as youâ€™d drop it into `core.py`, so you donâ€™t have to reconstruct any of this while mid-coding.
Yeah, this is absolutely good to go â€” and itâ€™s very, very close to what Iâ€™d have written myself. ğŸ§ âš™ï¸

Let me just sanity-check it and sprinkle a few micro-tweaks so you donâ€™t get bitten by edge cases.

---

## 1. Conceptual check: does this match â€œÃ‰mile coherenceâ€?

Your updated bit:

> **Coherence = cos(Iâ‚œ, Iâ‚€) Ã— TE(Iâ†’B) / (TE(Iâ†’B) + TE(othersâ†’B))**

And the implementation:

* `cos(Iâ‚œ, Iâ‚€)` via `initial_vector.to_array()` and `vector.to_array()`.
* TE ratio via lag-1 MI proxy over:

  * `identity_history` (scalar identity),
  * `behavior_history` (scalar behavior),
  * `others_history` (scalar mean behavior of others).

Interpretation:

* High cosine, high TE(Iâ†’B) â†’ â€œauthentic coherenceâ€.
* High cosine, low TE(Iâ†’B) â†’ â€œconformist coherenceâ€.
* Low cosine, high TE(Iâ†’B) â†’ â€œauthentically driftingâ€.
* Low cosine, low TE(Iâ†’B) â†’ â€œdragged away from self by fieldâ€.

That is exactly the antifinite â€œam I living my own surplus or being dragged?â€ structure weâ€™ve been circling for months. âœ…

---

## 2. TE implementation: a couple of tiny nits

Claudeâ€™s `transfer_entropy.py` stub is totally serviceable as a first-pass proxy. Just two tiny things Iâ€™d be aware of:

1. **Uniform series handling**

They already guard `_discretize` with:

```python
if len(series) == 0 or series.max() == series.min():
    return np.zeros_like(series, dtype=int)
```

ğŸ‘ That avoids NaNs when thereâ€™s no variation.

2. **Min length & â€œ1.0 means unknownâ€**

Youâ€™re using:

```python
if len(...) < min_len:
    return 1.0
```

and similar when `denom <= 1e-8`.

Thatâ€™s fine, but just remember in early rounds:

* `te_ratio` will be 1.0 by construction, not because identity is truly fully autonomous.
* Thatâ€™s okay â€” you can interpret the early rounds as â€œcoherence = pure cosineâ€ until history accumulates.

You might later want to log a flag like `te_valid: bool` if you want to distinguish â€œtrue 1.0â€ from â€œunknown 1.0â€, but not urgent.

---

## 3. IdentityCore + runner wiring: looks correct

### History deques + update method

This bit is good:

```python
_identity_history: Deque[float] = field(default_factory=lambda: deque(maxlen=50))
_behavior_history: Deque[float] = field(default_factory=lambda: deque(maxlen=50))
_others_behavior_history: Deque[float] = field(default_factory=lambda: deque(maxlen=50))
_te_ratio: float = 1.0
```

and

```python
def update_te_histories(...):
    self._identity_history.append(identity_scalar)
    self._behavior_history.append(behavior_scalar)
    self._others_behavior_history.append(others_behavior_scalar)
```

Thatâ€™s exactly what you want.

### Runner side

This is the only place you need to be a bit careful with *order*:

```python
for agent_id, ic in self.identity_cores.items():
    identity_scalar = ic.compute_delta_I()
    behavior_scalar = agent_engagements.get(agent_id, 0.0)
    other_engagements = [v for k, v in agent_engagements.items() if k != agent_id]
    others_scalar = np.mean(other_engagements) if other_engagements else 0.0

    ic.update_te_histories(identity_scalar, behavior_scalar, others_scalar)
```

Then later:

```python
identity_state['te_ratio'] = ic._te_ratio
```

Make sure that:

* `compute_coherence()` (which calls `te_ratio_proxy`) runs **after** `update_te_histories` for that round, not before.
* Or, equivalently: you call `compute_coherence()` *next round* and log the value then. Either convention is fine; just be consistent.

But structurally, itâ€™s right: TE has the previous time steps, uses lag-1 MI, and coherence is modulated accordingly.

---

## 4. One last reminder about IdentityVector

Everything above is predicated on the N-dim refactor using a **stable dimension order** and having `compute_delta_I()` work over that. So my only â€œplease donâ€™t forgetâ€ is:

* Define a canonical dimension list (e.g. `["engagement", "institutional_faith", "ideology", "partisanship", "sociogeographic", "social_friction", "tie_to_place"]`).
* Use that list in `to_array()` instead of `sorted(self.values.keys())`.
* Add property **setters** for `engagement`, `institutional_faith`, etc., so any old code that assigns into the vector still works.

That way, when TE uses `compute_delta_I()`, it will be genuinely 6D, not â€œwhatever order Python gives youâ€.

---

## 5. Answering your actual question

> Good to go?

Yes. With:

* fixed dim order in `IdentityVector`,
* property setters on the main dims,
* and the runner calling `update_te_histories` before `compute_coherence` is used/logged,

this TE plan is fully aligned with:

* your earlier theoretical spec,
* the QSE/Ã‰mile ethos,
* and the current state of the codebase Claude described.

If you implement it as written (plus those two IdentityVector polish points), youâ€™ll finally have:

> **Coherence = direction + autonomy**, not just â€œdid my vector move or not.â€

Which is exactly what youâ€™ve been trying to get back since the placeholder `te_ratio = 1.0` snuck in.
